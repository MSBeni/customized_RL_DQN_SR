{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/wau/keras-rl2.git\n",
    "# !cd keras-rl\n",
    "# !python install .\n",
    "# !py -m pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salimibeni\\anaconda3\\envs\\gymTF\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:150: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for step in range(200):\n",
    "    env.render(mode=\"human\")\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n\n",
    "n_obs = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 4)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                80        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,)+n_obs))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(n_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=20000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                             attr='eps',\n",
    "                             value_max=1.0,\n",
    "                             value_min=0.1,\n",
    "                             value_test=0.05,\n",
    "                             nb_steps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=n_actions, memory=memory, nb_steps_warmup=10, target_model_update=100, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salimibeni\\anaconda3\\envs\\gymTF\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "C:\\Users\\salimibeni\\anaconda3\\envs\\gymTF\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    29/20000: episode: 1, duration: 0.421s, episode steps:  29, steps per second:  69, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.457465, mae: 0.531780, mean_q: 0.131458, mean_eps: 0.999123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salimibeni\\anaconda3\\envs\\gymTF\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    81/20000: episode: 2, duration: 0.213s, episode steps:  52, steps per second: 245, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.404 [0.000, 1.000],  loss: 0.205831, mae: 0.557344, mean_q: 0.559785, mean_eps: 0.997548\n",
      "   112/20000: episode: 3, duration: 0.131s, episode steps:  31, steps per second: 236, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.209900, mae: 0.796274, mean_q: 1.139285, mean_eps: 0.995680\n",
      "   142/20000: episode: 4, duration: 0.125s, episode steps:  30, steps per second: 240, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.633 [0.000, 1.000],  loss: 0.117289, mae: 1.123798, mean_q: 2.019277, mean_eps: 0.994308\n",
      "   192/20000: episode: 5, duration: 0.203s, episode steps:  50, steps per second: 247, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.094990, mae: 1.122960, mean_q: 2.158511, mean_eps: 0.992507\n",
      "   219/20000: episode: 6, duration: 0.113s, episode steps:  27, steps per second: 239, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.282186, mae: 1.457116, mean_q: 2.451943, mean_eps: 0.990775\n",
      "   231/20000: episode: 7, duration: 0.052s, episode steps:  12, steps per second: 231, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.167137, mae: 1.714366, mean_q: 3.381605, mean_eps: 0.989897\n",
      "   242/20000: episode: 8, duration: 0.047s, episode steps:  11, steps per second: 236, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.198930, mae: 1.689251, mean_q: 3.305371, mean_eps: 0.989380\n",
      "   258/20000: episode: 9, duration: 0.068s, episode steps:  16, steps per second: 236, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.183045, mae: 1.662029, mean_q: 3.189516, mean_eps: 0.988773\n",
      "   277/20000: episode: 10, duration: 0.078s, episode steps:  19, steps per second: 243, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.147127, mae: 1.642232, mean_q: 3.195104, mean_eps: 0.987985\n",
      "   297/20000: episode: 11, duration: 0.083s, episode steps:  20, steps per second: 240, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.115919, mae: 1.625239, mean_q: 3.252172, mean_eps: 0.987107\n",
      "   321/20000: episode: 12, duration: 0.103s, episode steps:  24, steps per second: 234, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.425594, mae: 2.092958, mean_q: 3.751657, mean_eps: 0.986118\n",
      "   340/20000: episode: 13, duration: 0.081s, episode steps:  19, steps per second: 235, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.200720, mae: 2.230557, mean_q: 4.362073, mean_eps: 0.985150\n",
      "   377/20000: episode: 14, duration: 0.153s, episode steps:  37, steps per second: 242, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.432 [0.000, 1.000],  loss: 0.197523, mae: 2.148806, mean_q: 4.218109, mean_eps: 0.983890\n",
      "   396/20000: episode: 15, duration: 0.081s, episode steps:  19, steps per second: 235, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.216100, mae: 2.152458, mean_q: 4.174199, mean_eps: 0.982630\n",
      "   422/20000: episode: 16, duration: 0.112s, episode steps:  26, steps per second: 233, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 0.450341, mae: 2.567069, mean_q: 4.720241, mean_eps: 0.981618\n",
      "   441/20000: episode: 17, duration: 0.082s, episode steps:  19, steps per second: 231, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.279991, mae: 2.648026, mean_q: 5.102976, mean_eps: 0.980605\n",
      "   459/20000: episode: 18, duration: 0.078s, episode steps:  18, steps per second: 232, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.250233, mae: 2.632188, mean_q: 5.042967, mean_eps: 0.979772\n",
      "   472/20000: episode: 19, duration: 0.056s, episode steps:  13, steps per second: 231, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.242695, mae: 2.610123, mean_q: 5.075358, mean_eps: 0.979075\n",
      "   491/20000: episode: 20, duration: 0.081s, episode steps:  19, steps per second: 236, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.267100, mae: 2.613973, mean_q: 5.053121, mean_eps: 0.978355\n",
      "   515/20000: episode: 21, duration: 0.101s, episode steps:  24, steps per second: 237, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.435576, mae: 2.885334, mean_q: 5.349877, mean_eps: 0.977387\n",
      "   527/20000: episode: 22, duration: 0.053s, episode steps:  12, steps per second: 225, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.392204, mae: 3.204796, mean_q: 6.230392, mean_eps: 0.976577\n",
      "   539/20000: episode: 23, duration: 0.052s, episode steps:  12, steps per second: 231, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.458854, mae: 3.091220, mean_q: 5.827489, mean_eps: 0.976038\n",
      "   554/20000: episode: 24, duration: 0.064s, episode steps:  15, steps per second: 236, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.349876, mae: 3.066009, mean_q: 5.854647, mean_eps: 0.975430\n",
      "   567/20000: episode: 25, duration: 0.057s, episode steps:  13, steps per second: 230, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.388190, mae: 3.071749, mean_q: 5.961694, mean_eps: 0.974800\n",
      "   595/20000: episode: 26, duration: 0.117s, episode steps:  28, steps per second: 240, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.417113, mae: 3.089503, mean_q: 5.932962, mean_eps: 0.973877\n",
      "   604/20000: episode: 27, duration: 0.042s, episode steps:   9, steps per second: 213, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.567439, mae: 3.195446, mean_q: 5.846696, mean_eps: 0.973045\n",
      "   622/20000: episode: 28, duration: 0.076s, episode steps:  18, steps per second: 237, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.740985, mae: 3.533280, mean_q: 6.630495, mean_eps: 0.972438\n",
      "   634/20000: episode: 29, duration: 0.052s, episode steps:  12, steps per second: 231, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.481542, mae: 3.473804, mean_q: 6.645107, mean_eps: 0.971763\n",
      "   658/20000: episode: 30, duration: 0.100s, episode steps:  24, steps per second: 240, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.536062, mae: 3.482935, mean_q: 6.692007, mean_eps: 0.970952\n",
      "   674/20000: episode: 31, duration: 0.069s, episode steps:  16, steps per second: 233, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.480917, mae: 3.449203, mean_q: 6.583682, mean_eps: 0.970052\n",
      "   687/20000: episode: 32, duration: 0.055s, episode steps:  13, steps per second: 237, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.497736, mae: 3.466241, mean_q: 6.631398, mean_eps: 0.969400\n",
      "   714/20000: episode: 33, duration: 0.114s, episode steps:  27, steps per second: 237, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.552685, mae: 3.656370, mean_q: 6.879352, mean_eps: 0.968500\n",
      "   727/20000: episode: 34, duration: 0.055s, episode steps:  13, steps per second: 235, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.762310, mae: 3.923237, mean_q: 7.527453, mean_eps: 0.967600\n",
      "   738/20000: episode: 35, duration: 0.048s, episode steps:  11, steps per second: 228, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.400444, mae: 3.819840, mean_q: 7.399667, mean_eps: 0.967060\n",
      "   755/20000: episode: 36, duration: 0.075s, episode steps:  17, steps per second: 228, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.576422, mae: 3.904374, mean_q: 7.552479, mean_eps: 0.966430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   810/20000: episode: 37, duration: 0.228s, episode steps:  55, steps per second: 241, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.650830, mae: 3.929082, mean_q: 7.452799, mean_eps: 0.964810\n",
      "   824/20000: episode: 38, duration: 0.059s, episode steps:  14, steps per second: 237, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.629250, mae: 4.308128, mean_q: 8.314438, mean_eps: 0.963258\n",
      "   837/20000: episode: 39, duration: 0.056s, episode steps:  13, steps per second: 234, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.751082, mae: 4.236682, mean_q: 8.147776, mean_eps: 0.962650\n",
      "   855/20000: episode: 40, duration: 0.075s, episode steps:  18, steps per second: 240, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.675277, mae: 4.249873, mean_q: 8.201945, mean_eps: 0.961953\n",
      "   869/20000: episode: 41, duration: 0.061s, episode steps:  14, steps per second: 229, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.541761, mae: 4.205898, mean_q: 8.076064, mean_eps: 0.961233\n",
      "   883/20000: episode: 42, duration: 0.062s, episode steps:  14, steps per second: 227, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.874436, mae: 4.259880, mean_q: 8.056916, mean_eps: 0.960603\n",
      "   899/20000: episode: 43, duration: 0.070s, episode steps:  16, steps per second: 228, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.554923, mae: 4.243541, mean_q: 8.115212, mean_eps: 0.959928\n",
      "   914/20000: episode: 44, duration: 0.065s, episode steps:  15, steps per second: 229, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.960654, mae: 4.639750, mean_q: 8.686721, mean_eps: 0.959230\n",
      "   928/20000: episode: 45, duration: 0.061s, episode steps:  14, steps per second: 228, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.858348, mae: 4.744805, mean_q: 9.226928, mean_eps: 0.958578\n",
      "   938/20000: episode: 46, duration: 0.044s, episode steps:  10, steps per second: 227, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 1.008009, mae: 4.655588, mean_q: 8.884266, mean_eps: 0.958037\n",
      "   959/20000: episode: 47, duration: 0.087s, episode steps:  21, steps per second: 241, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.738164, mae: 4.723166, mean_q: 9.152656, mean_eps: 0.957340\n",
      "  1026/20000: episode: 48, duration: 0.276s, episode steps:  67, steps per second: 242, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.853891, mae: 4.888931, mean_q: 9.413035, mean_eps: 0.955360\n",
      "  1063/20000: episode: 49, duration: 0.151s, episode steps:  37, steps per second: 245, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.841776, mae: 5.174587, mean_q: 10.072986, mean_eps: 0.953020\n",
      "  1093/20000: episode: 50, duration: 0.125s, episode steps:  30, steps per second: 240, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.947147, mae: 5.111936, mean_q: 9.962804, mean_eps: 0.951512\n",
      "  1109/20000: episode: 51, duration: 0.074s, episode steps:  16, steps per second: 216, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.752060, mae: 5.356617, mean_q: 10.356973, mean_eps: 0.950477\n",
      "  1148/20000: episode: 52, duration: 0.159s, episode steps:  39, steps per second: 245, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 1.157341, mae: 5.665166, mean_q: 10.970536, mean_eps: 0.949240\n",
      "  1176/20000: episode: 53, duration: 0.116s, episode steps:  28, steps per second: 242, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.215075, mae: 5.688557, mean_q: 10.977391, mean_eps: 0.947732\n",
      "  1194/20000: episode: 54, duration: 0.076s, episode steps:  18, steps per second: 235, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.115119, mae: 5.578880, mean_q: 10.742453, mean_eps: 0.946697\n",
      "  1227/20000: episode: 55, duration: 0.139s, episode steps:  33, steps per second: 237, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 1.283615, mae: 5.912130, mean_q: 11.404011, mean_eps: 0.945550\n",
      "  1243/20000: episode: 56, duration: 0.069s, episode steps:  16, steps per second: 231, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 1.207510, mae: 5.957286, mean_q: 11.519006, mean_eps: 0.944447\n",
      "  1284/20000: episode: 57, duration: 0.167s, episode steps:  41, steps per second: 245, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 1.218122, mae: 6.033683, mean_q: 11.680305, mean_eps: 0.943165\n",
      "  1320/20000: episode: 58, duration: 0.151s, episode steps:  36, steps per second: 239, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.553444, mae: 6.183305, mean_q: 11.892166, mean_eps: 0.941433\n",
      "  1333/20000: episode: 59, duration: 0.056s, episode steps:  13, steps per second: 233, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.437881, mae: 6.314535, mean_q: 12.299676, mean_eps: 0.940330\n",
      "  1356/20000: episode: 60, duration: 0.095s, episode steps:  23, steps per second: 242, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 1.175554, mae: 6.367868, mean_q: 12.375984, mean_eps: 0.939520\n",
      "  1371/20000: episode: 61, duration: 0.064s, episode steps:  15, steps per second: 233, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 1.125114, mae: 6.332591, mean_q: 12.365942, mean_eps: 0.938665\n",
      "  1396/20000: episode: 62, duration: 0.107s, episode steps:  25, steps per second: 233, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.640 [0.000, 1.000],  loss: 1.537758, mae: 6.386206, mean_q: 12.378602, mean_eps: 0.937765\n",
      "  1407/20000: episode: 63, duration: 0.050s, episode steps:  11, steps per second: 221, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 2.438749, mae: 6.598994, mean_q: 12.463276, mean_eps: 0.936955\n",
      "  1428/20000: episode: 64, duration: 0.088s, episode steps:  21, steps per second: 238, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.638226, mae: 6.809975, mean_q: 13.264595, mean_eps: 0.936235\n",
      "  1467/20000: episode: 65, duration: 0.162s, episode steps:  39, steps per second: 241, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 1.806498, mae: 6.701188, mean_q: 13.045330, mean_eps: 0.934885\n",
      "  1500/20000: episode: 66, duration: 0.138s, episode steps:  33, steps per second: 238, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.114492, mae: 6.715559, mean_q: 13.254202, mean_eps: 0.933265\n",
      "  1510/20000: episode: 67, duration: 0.045s, episode steps:  10, steps per second: 221, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.521315, mae: 7.037466, mean_q: 13.669718, mean_eps: 0.932298\n",
      "  1535/20000: episode: 68, duration: 0.106s, episode steps:  25, steps per second: 235, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 1.868732, mae: 7.196512, mean_q: 14.137853, mean_eps: 0.931510\n",
      "  1552/20000: episode: 69, duration: 0.074s, episode steps:  17, steps per second: 230, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 1.362400, mae: 7.111032, mean_q: 14.035818, mean_eps: 0.930565\n",
      "  1574/20000: episode: 70, duration: 0.094s, episode steps:  22, steps per second: 233, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.590291, mae: 7.136018, mean_q: 14.098984, mean_eps: 0.929687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1642/20000: episode: 71, duration: 0.279s, episode steps:  68, steps per second: 243, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 1.532130, mae: 7.403654, mean_q: 14.621177, mean_eps: 0.927663\n",
      "  1678/20000: episode: 72, duration: 0.147s, episode steps:  36, steps per second: 245, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1.672627, mae: 7.567540, mean_q: 14.981049, mean_eps: 0.925322\n",
      "  1700/20000: episode: 73, duration: 0.096s, episode steps:  22, steps per second: 230, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.585867, mae: 7.596769, mean_q: 14.793170, mean_eps: 0.924018\n",
      "  1731/20000: episode: 74, duration: 0.129s, episode steps:  31, steps per second: 240, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 2.396654, mae: 8.070150, mean_q: 15.897154, mean_eps: 0.922825\n",
      "  1754/20000: episode: 75, duration: 0.096s, episode steps:  23, steps per second: 239, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 1.540494, mae: 8.123110, mean_q: 16.188439, mean_eps: 0.921610\n",
      "  1779/20000: episode: 76, duration: 0.105s, episode steps:  25, steps per second: 237, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 2.302144, mae: 8.075513, mean_q: 15.940619, mean_eps: 0.920530\n",
      "  1840/20000: episode: 77, duration: 0.251s, episode steps:  61, steps per second: 243, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 2.086682, mae: 8.283584, mean_q: 16.286656, mean_eps: 0.918595\n",
      "  1857/20000: episode: 78, duration: 0.074s, episode steps:  17, steps per second: 230, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 1.762252, mae: 8.349849, mean_q: 16.602358, mean_eps: 0.916840\n",
      "  1873/20000: episode: 79, duration: 0.068s, episode steps:  16, steps per second: 234, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.082744, mae: 8.317920, mean_q: 16.482527, mean_eps: 0.916098\n",
      "  1905/20000: episode: 80, duration: 0.134s, episode steps:  32, steps per second: 240, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 1.646029, mae: 8.398022, mean_q: 16.701720, mean_eps: 0.915018\n",
      "  1921/20000: episode: 81, duration: 0.069s, episode steps:  16, steps per second: 232, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.295994, mae: 8.840731, mean_q: 17.624638, mean_eps: 0.913938\n",
      "  1944/20000: episode: 82, duration: 0.098s, episode steps:  23, steps per second: 234, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 1.142656, mae: 8.723364, mean_q: 17.489685, mean_eps: 0.913060\n",
      "  1956/20000: episode: 83, duration: 0.052s, episode steps:  12, steps per second: 231, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.520694, mae: 8.506269, mean_q: 17.053462, mean_eps: 0.912272\n",
      "  1968/20000: episode: 84, duration: 0.052s, episode steps:  12, steps per second: 229, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 2.413417, mae: 8.783580, mean_q: 17.468432, mean_eps: 0.911732\n",
      "  2001/20000: episode: 85, duration: 0.138s, episode steps:  33, steps per second: 238, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.148334, mae: 8.733571, mean_q: 17.319079, mean_eps: 0.910720\n",
      "  2019/20000: episode: 86, duration: 0.078s, episode steps:  18, steps per second: 231, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 2.742949, mae: 9.166866, mean_q: 17.991524, mean_eps: 0.909573\n",
      "  2035/20000: episode: 87, duration: 0.068s, episode steps:  16, steps per second: 234, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.049831, mae: 9.302256, mean_q: 18.579636, mean_eps: 0.908807\n",
      "  2057/20000: episode: 88, duration: 0.092s, episode steps:  22, steps per second: 238, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 2.494535, mae: 9.037051, mean_q: 17.864164, mean_eps: 0.907952\n",
      "  2082/20000: episode: 89, duration: 0.110s, episode steps:  25, steps per second: 226, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.020974, mae: 9.252423, mean_q: 18.439602, mean_eps: 0.906895\n",
      "  2106/20000: episode: 90, duration: 0.106s, episode steps:  24, steps per second: 227, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 2.603888, mae: 9.233106, mean_q: 18.329876, mean_eps: 0.905793\n",
      "  2119/20000: episode: 91, duration: 0.056s, episode steps:  13, steps per second: 233, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 2.255035, mae: 9.662079, mean_q: 19.341984, mean_eps: 0.904960\n",
      "  2136/20000: episode: 92, duration: 0.073s, episode steps:  17, steps per second: 234, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 2.834428, mae: 9.432465, mean_q: 18.806372, mean_eps: 0.904285\n",
      "  2249/20000: episode: 93, duration: 0.462s, episode steps: 113, steps per second: 245, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 2.495112, mae: 9.803707, mean_q: 19.591920, mean_eps: 0.901360\n",
      "  2267/20000: episode: 94, duration: 0.076s, episode steps:  18, steps per second: 237, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 4.218873, mae: 10.067157, mean_q: 19.867786, mean_eps: 0.898412\n",
      "  2278/20000: episode: 95, duration: 0.047s, episode steps:  11, steps per second: 233, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 2.846872, mae: 10.024618, mean_q: 20.010117, mean_eps: 0.897760\n",
      "  2302/20000: episode: 96, duration: 0.103s, episode steps:  24, steps per second: 234, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 2.656588, mae: 10.154651, mean_q: 20.145684, mean_eps: 0.896972\n",
      "  2324/20000: episode: 97, duration: 0.094s, episode steps:  22, steps per second: 235, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.520698, mae: 10.382829, mean_q: 20.774676, mean_eps: 0.895938\n",
      "  2345/20000: episode: 98, duration: 0.087s, episode steps:  21, steps per second: 240, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 2.909199, mae: 10.428040, mean_q: 20.820276, mean_eps: 0.894970\n",
      "  2372/20000: episode: 99, duration: 0.111s, episode steps:  27, steps per second: 243, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.162125, mae: 10.517907, mean_q: 21.186764, mean_eps: 0.893890\n",
      "  2386/20000: episode: 100, duration: 0.060s, episode steps:  14, steps per second: 234, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 2.043240, mae: 10.539567, mean_q: 21.292831, mean_eps: 0.892968\n",
      "  2403/20000: episode: 101, duration: 0.073s, episode steps:  17, steps per second: 233, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 2.847460, mae: 10.498504, mean_q: 21.000378, mean_eps: 0.892270\n",
      "  2418/20000: episode: 102, duration: 0.064s, episode steps:  15, steps per second: 233, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 2.845945, mae: 10.742280, mean_q: 21.834299, mean_eps: 0.891550\n",
      "  2453/20000: episode: 103, duration: 0.146s, episode steps:  35, steps per second: 240, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.377068, mae: 10.815209, mean_q: 21.939440, mean_eps: 0.890425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2475/20000: episode: 104, duration: 0.095s, episode steps:  22, steps per second: 233, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 2.441390, mae: 10.873180, mean_q: 21.867598, mean_eps: 0.889143\n",
      "  2524/20000: episode: 105, duration: 0.201s, episode steps:  49, steps per second: 244, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 2.577660, mae: 10.886467, mean_q: 21.879466, mean_eps: 0.887545\n",
      "  2557/20000: episode: 106, duration: 0.137s, episode steps:  33, steps per second: 241, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.707858, mae: 11.075387, mean_q: 22.391075, mean_eps: 0.885700\n",
      "  2580/20000: episode: 107, duration: 0.097s, episode steps:  23, steps per second: 237, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 2.649442, mae: 11.117284, mean_q: 22.503501, mean_eps: 0.884440\n",
      "  2598/20000: episode: 108, duration: 0.075s, episode steps:  18, steps per second: 239, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 2.680612, mae: 11.129065, mean_q: 22.521331, mean_eps: 0.883517\n",
      "  2619/20000: episode: 109, duration: 0.088s, episode steps:  21, steps per second: 239, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 2.802192, mae: 11.531475, mean_q: 23.163747, mean_eps: 0.882640\n",
      "  2629/20000: episode: 110, duration: 0.043s, episode steps:  10, steps per second: 232, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.515517, mae: 11.519759, mean_q: 23.404409, mean_eps: 0.881942\n",
      "  2646/20000: episode: 111, duration: 0.072s, episode steps:  17, steps per second: 236, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.853817, mae: 11.585406, mean_q: 23.222811, mean_eps: 0.881335\n",
      "  2667/20000: episode: 112, duration: 0.087s, episode steps:  21, steps per second: 241, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 2.552145, mae: 11.399144, mean_q: 23.247184, mean_eps: 0.880480\n",
      "  2685/20000: episode: 113, duration: 0.076s, episode steps:  18, steps per second: 236, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.051484, mae: 11.544735, mean_q: 23.467475, mean_eps: 0.879602\n",
      "  2697/20000: episode: 114, duration: 0.052s, episode steps:  12, steps per second: 231, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 4.545884, mae: 11.389650, mean_q: 22.895137, mean_eps: 0.878927\n",
      "  2709/20000: episode: 115, duration: 0.053s, episode steps:  12, steps per second: 225, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 2.483039, mae: 11.369881, mean_q: 22.975526, mean_eps: 0.878388\n",
      "  2724/20000: episode: 116, duration: 0.063s, episode steps:  15, steps per second: 237, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.788030, mae: 11.877479, mean_q: 24.119652, mean_eps: 0.877780\n",
      "  2750/20000: episode: 117, duration: 0.108s, episode steps:  26, steps per second: 241, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.466282, mae: 11.932698, mean_q: 24.222327, mean_eps: 0.876857\n",
      "  2775/20000: episode: 118, duration: 0.102s, episode steps:  25, steps per second: 245, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 3.108684, mae: 11.920893, mean_q: 24.222154, mean_eps: 0.875710\n",
      "  2817/20000: episode: 119, duration: 0.174s, episode steps:  42, steps per second: 242, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.083251, mae: 12.192757, mean_q: 24.844394, mean_eps: 0.874203\n",
      "  2833/20000: episode: 120, duration: 0.067s, episode steps:  16, steps per second: 237, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.365087, mae: 12.519601, mean_q: 25.576213, mean_eps: 0.872897\n",
      "  2846/20000: episode: 121, duration: 0.055s, episode steps:  13, steps per second: 234, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 3.176285, mae: 12.390514, mean_q: 25.318544, mean_eps: 0.872245\n",
      "  2892/20000: episode: 122, duration: 0.189s, episode steps:  46, steps per second: 244, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.129139, mae: 12.475502, mean_q: 25.372500, mean_eps: 0.870918\n",
      "  2910/20000: episode: 123, duration: 0.076s, episode steps:  18, steps per second: 235, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 3.784823, mae: 12.695705, mean_q: 25.674617, mean_eps: 0.869477\n",
      "  2956/20000: episode: 124, duration: 0.186s, episode steps:  46, steps per second: 247, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.441744, mae: 13.121698, mean_q: 26.570098, mean_eps: 0.868037\n",
      "  2996/20000: episode: 125, duration: 0.164s, episode steps:  40, steps per second: 243, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 3.009321, mae: 13.112371, mean_q: 26.667598, mean_eps: 0.866102\n",
      "  3012/20000: episode: 126, duration: 0.070s, episode steps:  16, steps per second: 229, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.998901, mae: 13.335106, mean_q: 27.077518, mean_eps: 0.864842\n",
      "  3075/20000: episode: 127, duration: 0.256s, episode steps:  63, steps per second: 246, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.689230, mae: 13.461212, mean_q: 27.394850, mean_eps: 0.863065\n",
      "  3092/20000: episode: 128, duration: 0.071s, episode steps:  17, steps per second: 239, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 4.272011, mae: 13.410892, mean_q: 27.152392, mean_eps: 0.861265\n",
      "  3137/20000: episode: 129, duration: 0.184s, episode steps:  45, steps per second: 244, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.946028, mae: 14.120461, mean_q: 28.694657, mean_eps: 0.859870\n",
      "  3160/20000: episode: 130, duration: 0.095s, episode steps:  23, steps per second: 242, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 3.401085, mae: 14.139099, mean_q: 28.881909, mean_eps: 0.858340\n",
      "  3174/20000: episode: 131, duration: 0.060s, episode steps:  14, steps per second: 234, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.171307, mae: 14.066749, mean_q: 28.669680, mean_eps: 0.857508\n",
      "  3189/20000: episode: 132, duration: 0.064s, episode steps:  15, steps per second: 236, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4.072633, mae: 13.900407, mean_q: 28.201167, mean_eps: 0.856855\n",
      "  3218/20000: episode: 133, duration: 0.124s, episode steps:  29, steps per second: 234, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.379 [0.000, 1.000],  loss: 4.513616, mae: 14.354394, mean_q: 29.171961, mean_eps: 0.855865\n",
      "  3323/20000: episode: 134, duration: 0.424s, episode steps: 105, steps per second: 248, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.266826, mae: 14.740709, mean_q: 30.170591, mean_eps: 0.852850\n",
      "  3332/20000: episode: 135, duration: 0.040s, episode steps:   9, steps per second: 224, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 3.154308, mae: 14.847962, mean_q: 30.391105, mean_eps: 0.850285\n",
      "  3344/20000: episode: 136, duration: 0.052s, episode steps:  12, steps per second: 233, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 4.896637, mae: 14.874278, mean_q: 30.393476, mean_eps: 0.849813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3371/20000: episode: 137, duration: 0.110s, episode steps:  27, steps per second: 245, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 3.962578, mae: 15.290570, mean_q: 31.157331, mean_eps: 0.848935\n",
      "  3384/20000: episode: 138, duration: 0.057s, episode steps:  13, steps per second: 229, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 3.035384, mae: 15.222205, mean_q: 30.916968, mean_eps: 0.848035\n",
      "  3410/20000: episode: 139, duration: 0.108s, episode steps:  26, steps per second: 242, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 3.880481, mae: 15.405844, mean_q: 31.254695, mean_eps: 0.847158\n",
      "  3462/20000: episode: 140, duration: 0.212s, episode steps:  52, steps per second: 245, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.442 [0.000, 1.000],  loss: 2.935093, mae: 15.440858, mean_q: 31.661230, mean_eps: 0.845402\n",
      "  3487/20000: episode: 141, duration: 0.103s, episode steps:  25, steps per second: 242, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.663543, mae: 15.590596, mean_q: 32.043809, mean_eps: 0.843670\n",
      "  3503/20000: episode: 142, duration: 0.069s, episode steps:  16, steps per second: 231, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 1.874735, mae: 16.063395, mean_q: 32.803880, mean_eps: 0.842747\n",
      "  3523/20000: episode: 143, duration: 0.084s, episode steps:  20, steps per second: 238, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.535491, mae: 16.154212, mean_q: 32.955247, mean_eps: 0.841937\n",
      "  3539/20000: episode: 144, duration: 0.068s, episode steps:  16, steps per second: 235, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 4.688370, mae: 16.247727, mean_q: 33.428935, mean_eps: 0.841128\n",
      "  3554/20000: episode: 145, duration: 0.064s, episode steps:  15, steps per second: 235, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 3.609073, mae: 15.899472, mean_q: 32.573595, mean_eps: 0.840430\n",
      "  3604/20000: episode: 146, duration: 0.206s, episode steps:  50, steps per second: 243, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 3.781597, mae: 16.184024, mean_q: 33.116245, mean_eps: 0.838968\n",
      "  3671/20000: episode: 147, duration: 0.269s, episode steps:  67, steps per second: 249, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.803614, mae: 16.568619, mean_q: 33.922491, mean_eps: 0.836335\n",
      "  3729/20000: episode: 148, duration: 0.238s, episode steps:  58, steps per second: 244, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 3.695445, mae: 16.935048, mean_q: 34.737140, mean_eps: 0.833522\n",
      "  3744/20000: episode: 149, duration: 0.065s, episode steps:  15, steps per second: 232, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4.176544, mae: 17.132374, mean_q: 35.187048, mean_eps: 0.831880\n",
      "  3766/20000: episode: 150, duration: 0.091s, episode steps:  22, steps per second: 241, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.022419, mae: 17.146590, mean_q: 35.315663, mean_eps: 0.831047\n",
      "  3783/20000: episode: 151, duration: 0.073s, episode steps:  17, steps per second: 234, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 5.943793, mae: 17.109019, mean_q: 34.906737, mean_eps: 0.830170\n",
      "  3820/20000: episode: 152, duration: 0.152s, episode steps:  37, steps per second: 244, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 3.273160, mae: 17.638732, mean_q: 36.291532, mean_eps: 0.828955\n",
      "  3832/20000: episode: 153, duration: 0.052s, episode steps:  12, steps per second: 231, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 2.267905, mae: 17.645625, mean_q: 36.457789, mean_eps: 0.827853\n",
      "  3846/20000: episode: 154, duration: 0.061s, episode steps:  14, steps per second: 229, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 6.175574, mae: 17.785078, mean_q: 36.299150, mean_eps: 0.827267\n",
      "  3909/20000: episode: 155, duration: 0.258s, episode steps:  63, steps per second: 244, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.587 [0.000, 1.000],  loss: 3.736012, mae: 18.043704, mean_q: 37.125830, mean_eps: 0.825535\n",
      "  3950/20000: episode: 156, duration: 0.167s, episode steps:  41, steps per second: 246, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 4.161791, mae: 18.394966, mean_q: 37.739881, mean_eps: 0.823195\n",
      "  4002/20000: episode: 157, duration: 0.213s, episode steps:  52, steps per second: 244, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 4.982726, mae: 18.333926, mean_q: 37.625959, mean_eps: 0.821103\n",
      "  4131/20000: episode: 158, duration: 0.517s, episode steps: 129, steps per second: 249, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 4.114016, mae: 19.159157, mean_q: 39.309427, mean_eps: 0.817030\n",
      "  4168/20000: episode: 159, duration: 0.150s, episode steps:  37, steps per second: 246, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 4.798550, mae: 19.763486, mean_q: 40.568206, mean_eps: 0.813295\n",
      "  4188/20000: episode: 160, duration: 0.086s, episode steps:  20, steps per second: 233, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 3.932727, mae: 20.225156, mean_q: 41.394538, mean_eps: 0.812012\n",
      "  4229/20000: episode: 161, duration: 0.167s, episode steps:  41, steps per second: 245, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.366 [0.000, 1.000],  loss: 5.990103, mae: 20.271024, mean_q: 41.564116, mean_eps: 0.810640\n",
      "  4259/20000: episode: 162, duration: 0.134s, episode steps:  30, steps per second: 224, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 6.725615, mae: 20.417577, mean_q: 41.437271, mean_eps: 0.809042\n",
      "  4286/20000: episode: 163, duration: 0.112s, episode steps:  27, steps per second: 240, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 6.361017, mae: 20.517071, mean_q: 41.903768, mean_eps: 0.807760\n",
      "  4307/20000: episode: 164, duration: 0.088s, episode steps:  21, steps per second: 237, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 5.395572, mae: 20.720630, mean_q: 42.322339, mean_eps: 0.806680\n",
      "  4327/20000: episode: 165, duration: 0.084s, episode steps:  20, steps per second: 237, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 6.588727, mae: 21.428790, mean_q: 43.593482, mean_eps: 0.805758\n",
      "  4398/20000: episode: 166, duration: 0.289s, episode steps:  71, steps per second: 245, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.437 [0.000, 1.000],  loss: 6.647288, mae: 21.080417, mean_q: 42.945512, mean_eps: 0.803710\n",
      "  4414/20000: episode: 167, duration: 0.068s, episode steps:  16, steps per second: 235, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 6.644633, mae: 20.556437, mean_q: 42.039512, mean_eps: 0.801752\n",
      "  4439/20000: episode: 168, duration: 0.103s, episode steps:  25, steps per second: 244, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 7.183277, mae: 21.588725, mean_q: 43.664406, mean_eps: 0.800830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4492/20000: episode: 169, duration: 0.215s, episode steps:  53, steps per second: 246, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 4.858346, mae: 21.387195, mean_q: 43.792524, mean_eps: 0.799075\n",
      "  4508/20000: episode: 170, duration: 0.069s, episode steps:  16, steps per second: 232, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 5.669667, mae: 21.444256, mean_q: 43.990980, mean_eps: 0.797522\n",
      "  4528/20000: episode: 171, duration: 0.083s, episode steps:  20, steps per second: 240, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 3.594768, mae: 21.500418, mean_q: 44.338314, mean_eps: 0.796712\n",
      "  4545/20000: episode: 172, duration: 0.072s, episode steps:  17, steps per second: 237, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 4.334969, mae: 21.250595, mean_q: 43.814192, mean_eps: 0.795880\n",
      "  4621/20000: episode: 173, duration: 0.308s, episode steps:  76, steps per second: 247, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.592 [0.000, 1.000],  loss: 5.787913, mae: 21.792328, mean_q: 44.686081, mean_eps: 0.793787\n",
      "  4698/20000: episode: 174, duration: 0.310s, episode steps:  77, steps per second: 248, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 5.918077, mae: 22.498513, mean_q: 46.148870, mean_eps: 0.790345\n",
      "  4743/20000: episode: 175, duration: 0.186s, episode steps:  45, steps per second: 242, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 5.854047, mae: 22.798248, mean_q: 46.960908, mean_eps: 0.787600\n",
      "  4761/20000: episode: 176, duration: 0.077s, episode steps:  18, steps per second: 233, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 5.650631, mae: 23.133544, mean_q: 47.434203, mean_eps: 0.786183\n",
      "  4891/20000: episode: 177, duration: 0.522s, episode steps: 130, steps per second: 249, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 5.956807, mae: 23.098244, mean_q: 47.502431, mean_eps: 0.782853\n",
      "  4943/20000: episode: 178, duration: 0.211s, episode steps:  52, steps per second: 247, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5.031529, mae: 23.719958, mean_q: 48.739483, mean_eps: 0.778758\n",
      "  4967/20000: episode: 179, duration: 0.100s, episode steps:  24, steps per second: 240, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 7.634929, mae: 23.978003, mean_q: 48.935241, mean_eps: 0.777048\n",
      "  5012/20000: episode: 180, duration: 0.186s, episode steps:  45, steps per second: 242, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.578 [0.000, 1.000],  loss: 4.792022, mae: 24.080027, mean_q: 49.588364, mean_eps: 0.775495\n",
      "  5050/20000: episode: 181, duration: 0.155s, episode steps:  38, steps per second: 245, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 7.315973, mae: 24.727133, mean_q: 50.704252, mean_eps: 0.773627\n",
      "  5102/20000: episode: 182, duration: 0.212s, episode steps:  52, steps per second: 245, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.492046, mae: 24.767444, mean_q: 50.734951, mean_eps: 0.771602\n",
      "  5121/20000: episode: 183, duration: 0.080s, episode steps:  19, steps per second: 238, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 5.355780, mae: 24.862046, mean_q: 51.019614, mean_eps: 0.770005\n",
      "  5191/20000: episode: 184, duration: 0.283s, episode steps:  70, steps per second: 248, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 6.646057, mae: 25.079638, mean_q: 51.513947, mean_eps: 0.768002\n",
      "  5264/20000: episode: 185, duration: 0.298s, episode steps:  73, steps per second: 245, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 7.052488, mae: 25.656500, mean_q: 52.677890, mean_eps: 0.764785\n",
      "  5316/20000: episode: 186, duration: 0.214s, episode steps:  52, steps per second: 243, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 8.149122, mae: 25.986357, mean_q: 53.109679, mean_eps: 0.761972\n",
      "  5336/20000: episode: 187, duration: 0.085s, episode steps:  20, steps per second: 237, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 5.056610, mae: 25.960385, mean_q: 53.166878, mean_eps: 0.760352\n",
      "  5415/20000: episode: 188, duration: 0.322s, episode steps:  79, steps per second: 245, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 6.482687, mae: 26.288185, mean_q: 54.096862, mean_eps: 0.758125\n",
      "  5463/20000: episode: 189, duration: 0.195s, episode steps:  48, steps per second: 246, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 9.280947, mae: 27.176995, mean_q: 55.568051, mean_eps: 0.755267\n",
      "  5478/20000: episode: 190, duration: 0.064s, episode steps:  15, steps per second: 235, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 6.233072, mae: 26.874884, mean_q: 55.346921, mean_eps: 0.753850\n",
      "  5488/20000: episode: 191, duration: 0.044s, episode steps:  10, steps per second: 229, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 11.407284, mae: 27.117849, mean_q: 55.614633, mean_eps: 0.753287\n",
      "  5538/20000: episode: 192, duration: 0.205s, episode steps:  50, steps per second: 244, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.444670, mae: 27.309600, mean_q: 55.931584, mean_eps: 0.751938\n",
      "  5574/20000: episode: 193, duration: 0.148s, episode steps:  36, steps per second: 244, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 7.520031, mae: 27.626610, mean_q: 56.657983, mean_eps: 0.750003\n",
      "  5644/20000: episode: 194, duration: 0.286s, episode steps:  70, steps per second: 245, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 7.531383, mae: 28.214086, mean_q: 57.812560, mean_eps: 0.747618\n",
      "  5654/20000: episode: 195, duration: 0.044s, episode steps:  10, steps per second: 227, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 5.117404, mae: 28.217622, mean_q: 58.562988, mean_eps: 0.745818\n",
      "  5677/20000: episode: 196, duration: 0.095s, episode steps:  23, steps per second: 242, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 7.886280, mae: 28.674304, mean_q: 58.977354, mean_eps: 0.745075\n",
      "  5718/20000: episode: 197, duration: 0.171s, episode steps:  41, steps per second: 240, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.415 [0.000, 1.000],  loss: 5.857887, mae: 28.784704, mean_q: 59.279877, mean_eps: 0.743635\n",
      "  5767/20000: episode: 198, duration: 0.204s, episode steps:  49, steps per second: 241, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 9.918916, mae: 28.828535, mean_q: 59.163423, mean_eps: 0.741610\n",
      "  5819/20000: episode: 199, duration: 0.214s, episode steps:  52, steps per second: 243, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.442 [0.000, 1.000],  loss: 6.732424, mae: 29.192002, mean_q: 60.036093, mean_eps: 0.739338\n",
      "  5841/20000: episode: 200, duration: 0.092s, episode steps:  22, steps per second: 239, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 8.287516, mae: 28.983985, mean_q: 59.277648, mean_eps: 0.737672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5923/20000: episode: 201, duration: 0.333s, episode steps:  82, steps per second: 246, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 8.152236, mae: 30.180516, mean_q: 61.952602, mean_eps: 0.735333\n",
      "  5936/20000: episode: 202, duration: 0.056s, episode steps:  13, steps per second: 231, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 13.687941, mae: 30.388956, mean_q: 62.244693, mean_eps: 0.733195\n",
      "  5961/20000: episode: 203, duration: 0.104s, episode steps:  25, steps per second: 239, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.360 [0.000, 1.000],  loss: 9.578004, mae: 31.174235, mean_q: 63.818448, mean_eps: 0.732340\n",
      "  6018/20000: episode: 204, duration: 0.235s, episode steps:  57, steps per second: 243, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 8.540579, mae: 30.655606, mean_q: 62.656286, mean_eps: 0.730495\n",
      "  6032/20000: episode: 205, duration: 0.060s, episode steps:  14, steps per second: 233, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 11.805311, mae: 31.143766, mean_q: 63.839921, mean_eps: 0.728898\n",
      "  6071/20000: episode: 206, duration: 0.160s, episode steps:  39, steps per second: 244, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 9.642355, mae: 31.322441, mean_q: 64.144928, mean_eps: 0.727705\n",
      "  6099/20000: episode: 207, duration: 0.115s, episode steps:  28, steps per second: 243, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 14.109009, mae: 30.748766, mean_q: 63.047006, mean_eps: 0.726198\n",
      "  6122/20000: episode: 208, duration: 0.098s, episode steps:  23, steps per second: 234, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 8.049966, mae: 31.559412, mean_q: 64.587745, mean_eps: 0.725050\n",
      "  6177/20000: episode: 209, duration: 0.222s, episode steps:  55, steps per second: 248, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 8.874032, mae: 31.785162, mean_q: 65.298334, mean_eps: 0.723295\n",
      "  6197/20000: episode: 210, duration: 0.085s, episode steps:  20, steps per second: 236, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 10.531384, mae: 31.413828, mean_q: 64.802388, mean_eps: 0.721607\n",
      "  6242/20000: episode: 211, duration: 0.185s, episode steps:  45, steps per second: 244, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 8.807512, mae: 31.730019, mean_q: 65.076613, mean_eps: 0.720145\n",
      "  6259/20000: episode: 212, duration: 0.071s, episode steps:  17, steps per second: 240, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 10.422898, mae: 31.752636, mean_q: 65.435377, mean_eps: 0.718750\n",
      "  6306/20000: episode: 213, duration: 0.194s, episode steps:  47, steps per second: 242, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 11.079183, mae: 31.799940, mean_q: 65.400951, mean_eps: 0.717310\n",
      "  6333/20000: episode: 214, duration: 0.111s, episode steps:  27, steps per second: 243, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 7.764782, mae: 32.086805, mean_q: 65.980688, mean_eps: 0.715645\n",
      "  6350/20000: episode: 215, duration: 0.071s, episode steps:  17, steps per second: 241, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 9.612528, mae: 32.397981, mean_q: 66.823913, mean_eps: 0.714655\n",
      "  6395/20000: episode: 216, duration: 0.184s, episode steps:  45, steps per second: 245, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 7.375887, mae: 32.625390, mean_q: 67.160850, mean_eps: 0.713260\n",
      "  6432/20000: episode: 217, duration: 0.153s, episode steps:  37, steps per second: 242, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 13.097755, mae: 33.558898, mean_q: 68.644637, mean_eps: 0.711415\n",
      "  6501/20000: episode: 218, duration: 0.279s, episode steps:  69, steps per second: 248, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 8.501712, mae: 33.130203, mean_q: 68.041731, mean_eps: 0.709030\n",
      "  6540/20000: episode: 219, duration: 0.161s, episode steps:  39, steps per second: 242, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 12.337932, mae: 33.712406, mean_q: 69.033517, mean_eps: 0.706600\n",
      "  6568/20000: episode: 220, duration: 0.116s, episode steps:  28, steps per second: 241, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 9.469117, mae: 34.151289, mean_q: 70.083099, mean_eps: 0.705092\n",
      "  6654/20000: episode: 221, duration: 0.351s, episode steps:  86, steps per second: 245, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 11.152576, mae: 34.522930, mean_q: 70.852952, mean_eps: 0.702527\n",
      "  6678/20000: episode: 222, duration: 0.101s, episode steps:  24, steps per second: 238, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 10.303744, mae: 34.616866, mean_q: 70.680198, mean_eps: 0.700052\n",
      "  6695/20000: episode: 223, duration: 0.073s, episode steps:  17, steps per second: 234, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 9.719796, mae: 33.730197, mean_q: 69.672541, mean_eps: 0.699130\n",
      "  6870/20000: episode: 224, duration: 0.705s, episode steps: 175, steps per second: 248, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 13.345873, mae: 35.831677, mean_q: 73.357861, mean_eps: 0.694810\n",
      "  6979/20000: episode: 225, duration: 0.443s, episode steps: 109, steps per second: 246, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 14.234158, mae: 36.391225, mean_q: 74.336703, mean_eps: 0.688420\n",
      "  7078/20000: episode: 226, duration: 0.403s, episode steps:  99, steps per second: 246, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 11.398461, mae: 36.904186, mean_q: 75.834018, mean_eps: 0.683740\n",
      "  7138/20000: episode: 227, duration: 0.244s, episode steps:  60, steps per second: 246, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 17.099379, mae: 37.397465, mean_q: 76.520179, mean_eps: 0.680163\n",
      "  7189/20000: episode: 228, duration: 0.207s, episode steps:  51, steps per second: 246, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 14.556410, mae: 37.538943, mean_q: 76.924888, mean_eps: 0.677665\n",
      "  7220/20000: episode: 229, duration: 0.130s, episode steps:  31, steps per second: 239, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.613 [0.000, 1.000],  loss: 12.006212, mae: 39.006652, mean_q: 79.875057, mean_eps: 0.675820\n",
      "  7238/20000: episode: 230, duration: 0.075s, episode steps:  18, steps per second: 239, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 20.625726, mae: 38.032988, mean_q: 78.271833, mean_eps: 0.674717\n",
      "  7273/20000: episode: 231, duration: 0.144s, episode steps:  35, steps per second: 243, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 18.386203, mae: 38.213562, mean_q: 78.591477, mean_eps: 0.673525\n",
      "  7316/20000: episode: 232, duration: 0.176s, episode steps:  43, steps per second: 244, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 16.152703, mae: 39.089629, mean_q: 79.936435, mean_eps: 0.671770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7352/20000: episode: 233, duration: 0.150s, episode steps:  36, steps per second: 239, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 14.330288, mae: 39.652785, mean_q: 81.214332, mean_eps: 0.669992\n",
      "  7462/20000: episode: 234, duration: 0.445s, episode steps: 110, steps per second: 247, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 11.256836, mae: 40.228696, mean_q: 82.666992, mean_eps: 0.666707\n",
      "  7526/20000: episode: 235, duration: 0.261s, episode steps:  64, steps per second: 245, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 16.599094, mae: 40.415166, mean_q: 82.854770, mean_eps: 0.662793\n",
      "  7575/20000: episode: 236, duration: 0.198s, episode steps:  49, steps per second: 247, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 21.681640, mae: 40.934468, mean_q: 84.137305, mean_eps: 0.660250\n",
      "  7618/20000: episode: 237, duration: 0.178s, episode steps:  43, steps per second: 242, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.442 [0.000, 1.000],  loss: 16.478767, mae: 41.106163, mean_q: 84.430249, mean_eps: 0.658180\n",
      "  7802/20000: episode: 238, duration: 0.737s, episode steps: 184, steps per second: 250, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 20.730001, mae: 42.321147, mean_q: 86.683842, mean_eps: 0.653073\n",
      "  7834/20000: episode: 239, duration: 0.131s, episode steps:  32, steps per second: 244, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 36.529428, mae: 43.949062, mean_q: 89.258716, mean_eps: 0.648212\n",
      "  7921/20000: episode: 240, duration: 0.354s, episode steps:  87, steps per second: 245, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 19.165180, mae: 43.699123, mean_q: 89.316077, mean_eps: 0.645535\n",
      "  7959/20000: episode: 241, duration: 0.154s, episode steps:  38, steps per second: 248, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 20.744680, mae: 44.935788, mean_q: 91.711217, mean_eps: 0.642722\n",
      "  7994/20000: episode: 242, duration: 0.144s, episode steps:  35, steps per second: 243, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 18.762624, mae: 44.240409, mean_q: 90.270723, mean_eps: 0.641080\n",
      "  8096/20000: episode: 243, duration: 0.414s, episode steps: 102, steps per second: 247, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 18.302950, mae: 45.387581, mean_q: 92.756281, mean_eps: 0.637997\n",
      "  8142/20000: episode: 244, duration: 0.189s, episode steps:  46, steps per second: 243, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 25.818262, mae: 45.111503, mean_q: 91.948362, mean_eps: 0.634668\n",
      "  8226/20000: episode: 245, duration: 0.353s, episode steps:  84, steps per second: 238, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 20.596839, mae: 45.320363, mean_q: 92.922852, mean_eps: 0.631742\n",
      "  8243/20000: episode: 246, duration: 0.072s, episode steps:  17, steps per second: 236, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 25.268053, mae: 45.712742, mean_q: 94.164395, mean_eps: 0.629470\n",
      "  8263/20000: episode: 247, duration: 0.084s, episode steps:  20, steps per second: 238, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 17.831244, mae: 45.852924, mean_q: 93.744568, mean_eps: 0.628637\n",
      "  8285/20000: episode: 248, duration: 0.093s, episode steps:  22, steps per second: 237, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 39.506569, mae: 46.302833, mean_q: 94.416052, mean_eps: 0.627692\n",
      "  8349/20000: episode: 249, duration: 0.263s, episode steps:  64, steps per second: 244, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 15.741525, mae: 47.063730, mean_q: 96.576411, mean_eps: 0.625757\n",
      "  8470/20000: episode: 250, duration: 0.494s, episode steps: 121, steps per second: 245, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 17.830198, mae: 47.737605, mean_q: 97.877615, mean_eps: 0.621595\n",
      "  8548/20000: episode: 251, duration: 0.319s, episode steps:  78, steps per second: 245, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 27.845348, mae: 47.953718, mean_q: 98.182392, mean_eps: 0.617117\n",
      "  8748/20000: episode: 252, duration: 0.805s, episode steps: 200, steps per second: 248, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 22.931918, mae: 49.047803, mean_q: 100.410709, mean_eps: 0.610862\n",
      "  8795/20000: episode: 253, duration: 0.192s, episode steps:  47, steps per second: 244, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 16.702015, mae: 49.610449, mean_q: 101.747208, mean_eps: 0.605305\n",
      "  8811/20000: episode: 254, duration: 0.071s, episode steps:  16, steps per second: 226, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 43.601968, mae: 49.488976, mean_q: 101.648226, mean_eps: 0.603887\n",
      "  8858/20000: episode: 255, duration: 0.193s, episode steps:  47, steps per second: 244, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 16.829018, mae: 49.516717, mean_q: 102.144768, mean_eps: 0.602470\n",
      "  8895/20000: episode: 256, duration: 0.154s, episode steps:  37, steps per second: 241, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 30.667409, mae: 50.610264, mean_q: 103.415689, mean_eps: 0.600580\n",
      "  8919/20000: episode: 257, duration: 0.101s, episode steps:  24, steps per second: 237, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 39.377231, mae: 48.963964, mean_q: 100.182205, mean_eps: 0.599208\n",
      "  9076/20000: episode: 258, duration: 0.631s, episode steps: 157, steps per second: 249, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 23.639342, mae: 50.748025, mean_q: 104.238824, mean_eps: 0.595135\n",
      "  9276/20000: episode: 259, duration: 0.809s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 24.505456, mae: 52.573019, mean_q: 108.052385, mean_eps: 0.587102\n",
      "  9404/20000: episode: 260, duration: 0.542s, episode steps: 128, steps per second: 236, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 32.395149, mae: 53.607178, mean_q: 109.903205, mean_eps: 0.579723\n",
      "  9508/20000: episode: 261, duration: 0.443s, episode steps: 104, steps per second: 235, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 29.531552, mae: 54.870143, mean_q: 112.479416, mean_eps: 0.574503\n",
      "  9588/20000: episode: 262, duration: 0.345s, episode steps:  80, steps per second: 232, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 29.745291, mae: 56.037334, mean_q: 114.357124, mean_eps: 0.570363\n",
      "  9729/20000: episode: 263, duration: 0.593s, episode steps: 141, steps per second: 238, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 29.117113, mae: 56.500107, mean_q: 115.729171, mean_eps: 0.565390\n",
      "  9864/20000: episode: 264, duration: 0.550s, episode steps: 135, steps per second: 246, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 26.464720, mae: 57.820658, mean_q: 118.416842, mean_eps: 0.559180\n",
      "  9905/20000: episode: 265, duration: 0.183s, episode steps:  41, steps per second: 224, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 21.166000, mae: 58.491283, mean_q: 119.721295, mean_eps: 0.555220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9945/20000: episode: 266, duration: 0.165s, episode steps:  40, steps per second: 242, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 39.292237, mae: 59.171135, mean_q: 120.784605, mean_eps: 0.553397\n",
      " 10097/20000: episode: 267, duration: 0.630s, episode steps: 152, steps per second: 241, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 32.848919, mae: 59.260728, mean_q: 121.411904, mean_eps: 0.549077\n",
      " 10111/20000: episode: 268, duration: 0.067s, episode steps:  14, steps per second: 207, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 23.158499, mae: 59.262721, mean_q: 122.455839, mean_eps: 0.545343\n",
      " 10131/20000: episode: 269, duration: 0.084s, episode steps:  20, steps per second: 239, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 36.525709, mae: 60.853590, mean_q: 124.926242, mean_eps: 0.544578\n",
      " 10195/20000: episode: 270, duration: 0.263s, episode steps:  64, steps per second: 244, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 46.111922, mae: 60.503865, mean_q: 123.859976, mean_eps: 0.542687\n",
      " 10256/20000: episode: 271, duration: 0.252s, episode steps:  61, steps per second: 242, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 21.041250, mae: 62.225447, mean_q: 127.023592, mean_eps: 0.539875\n",
      " 10302/20000: episode: 272, duration: 0.188s, episode steps:  46, steps per second: 244, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 30.697682, mae: 62.368823, mean_q: 127.901081, mean_eps: 0.537468\n",
      " 10334/20000: episode: 273, duration: 0.140s, episode steps:  32, steps per second: 229, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 33.967757, mae: 62.617845, mean_q: 128.630311, mean_eps: 0.535713\n",
      " 10527/20000: episode: 274, duration: 0.816s, episode steps: 193, steps per second: 237, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 33.272140, mae: 63.220967, mean_q: 129.421632, mean_eps: 0.530650\n",
      " 10599/20000: episode: 275, duration: 0.293s, episode steps:  72, steps per second: 246, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 37.732024, mae: 64.061301, mean_q: 131.258518, mean_eps: 0.524687\n",
      " 10694/20000: episode: 276, duration: 0.385s, episode steps:  95, steps per second: 247, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 33.624560, mae: 65.494353, mean_q: 133.930983, mean_eps: 0.520930\n",
      " 10725/20000: episode: 277, duration: 0.130s, episode steps:  31, steps per second: 239, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 24.168526, mae: 65.520258, mean_q: 134.844490, mean_eps: 0.518095\n",
      " 10801/20000: episode: 278, duration: 0.312s, episode steps:  76, steps per second: 244, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 38.164298, mae: 65.956849, mean_q: 135.323890, mean_eps: 0.515688\n",
      " 11001/20000: episode: 279, duration: 0.803s, episode steps: 200, steps per second: 249, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 39.443844, mae: 67.598994, mean_q: 138.524961, mean_eps: 0.509478\n",
      " 11094/20000: episode: 280, duration: 0.379s, episode steps:  93, steps per second: 245, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 42.442329, mae: 68.614552, mean_q: 140.081324, mean_eps: 0.502885\n",
      " 11294/20000: episode: 281, duration: 0.805s, episode steps: 200, steps per second: 248, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 42.294499, mae: 70.208875, mean_q: 143.802570, mean_eps: 0.496292\n",
      " 11465/20000: episode: 282, duration: 0.691s, episode steps: 171, steps per second: 247, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 53.919264, mae: 72.155679, mean_q: 147.277762, mean_eps: 0.487945\n",
      " 11665/20000: episode: 283, duration: 0.806s, episode steps: 200, steps per second: 248, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 46.086696, mae: 72.503791, mean_q: 148.145903, mean_eps: 0.479598\n",
      " 11865/20000: episode: 284, duration: 0.808s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 44.868173, mae: 75.009137, mean_q: 153.408043, mean_eps: 0.470598\n",
      " 12046/20000: episode: 285, duration: 0.729s, episode steps: 181, steps per second: 248, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 53.776598, mae: 76.268743, mean_q: 155.808694, mean_eps: 0.462025\n",
      " 12106/20000: episode: 286, duration: 0.246s, episode steps:  60, steps per second: 244, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 35.644134, mae: 78.050057, mean_q: 159.057404, mean_eps: 0.456602\n",
      " 12120/20000: episode: 287, duration: 0.061s, episode steps:  14, steps per second: 231, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 52.264906, mae: 78.909645, mean_q: 161.079147, mean_eps: 0.454937\n",
      " 12320/20000: episode: 288, duration: 0.810s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 54.167674, mae: 78.228406, mean_q: 159.889909, mean_eps: 0.450122\n",
      " 12509/20000: episode: 289, duration: 0.767s, episode steps: 189, steps per second: 247, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 56.072222, mae: 80.116778, mean_q: 163.586153, mean_eps: 0.441370\n",
      " 12709/20000: episode: 290, duration: 0.809s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 54.006554, mae: 80.539052, mean_q: 164.766494, mean_eps: 0.432617\n",
      " 12909/20000: episode: 291, duration: 0.806s, episode steps: 200, steps per second: 248, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 53.848579, mae: 81.782738, mean_q: 167.324650, mean_eps: 0.423617\n",
      " 13109/20000: episode: 292, duration: 0.819s, episode steps: 200, steps per second: 244, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 45.759619, mae: 83.374599, mean_q: 170.644204, mean_eps: 0.414617\n",
      " 13134/20000: episode: 293, duration: 0.106s, episode steps:  25, steps per second: 236, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 40.858502, mae: 84.091568, mean_q: 172.586732, mean_eps: 0.409555\n",
      " 13334/20000: episode: 294, duration: 0.811s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 51.152009, mae: 84.605973, mean_q: 173.124812, mean_eps: 0.404492\n",
      " 13383/20000: episode: 295, duration: 0.200s, episode steps:  49, steps per second: 245, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 76.860117, mae: 85.952938, mean_q: 175.666435, mean_eps: 0.398890\n",
      " 13583/20000: episode: 296, duration: 0.811s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 63.868718, mae: 86.337906, mean_q: 176.474277, mean_eps: 0.393287\n",
      " 13783/20000: episode: 297, duration: 0.814s, episode steps: 200, steps per second: 246, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 69.095443, mae: 88.366739, mean_q: 180.410841, mean_eps: 0.384287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13983/20000: episode: 298, duration: 0.810s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 67.966337, mae: 90.341464, mean_q: 184.698544, mean_eps: 0.375287\n",
      " 14183/20000: episode: 299, duration: 0.811s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 76.737011, mae: 91.662228, mean_q: 187.533691, mean_eps: 0.366287\n",
      " 14313/20000: episode: 300, duration: 0.529s, episode steps: 130, steps per second: 246, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 60.173648, mae: 92.557429, mean_q: 188.662822, mean_eps: 0.358862\n",
      " 14508/20000: episode: 301, duration: 0.789s, episode steps: 195, steps per second: 247, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 65.760029, mae: 93.782344, mean_q: 191.287731, mean_eps: 0.351550\n",
      " 14708/20000: episode: 302, duration: 0.811s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 92.476881, mae: 94.347627, mean_q: 191.750650, mean_eps: 0.342662\n",
      " 14908/20000: episode: 303, duration: 0.809s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 64.385346, mae: 96.503605, mean_q: 196.699148, mean_eps: 0.333662\n",
      " 15108/20000: episode: 304, duration: 0.821s, episode steps: 200, steps per second: 243, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 78.046813, mae: 98.072478, mean_q: 199.476824, mean_eps: 0.324662\n",
      " 15308/20000: episode: 305, duration: 0.826s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 97.660608, mae: 99.198264, mean_q: 201.681172, mean_eps: 0.315662\n",
      " 15508/20000: episode: 306, duration: 0.813s, episode steps: 200, steps per second: 246, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 96.054328, mae: 100.575892, mean_q: 204.694824, mean_eps: 0.306662\n",
      " 15708/20000: episode: 307, duration: 0.809s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 83.759168, mae: 101.496284, mean_q: 206.471338, mean_eps: 0.297662\n",
      " 15908/20000: episode: 308, duration: 0.809s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 94.648806, mae: 102.348014, mean_q: 208.340759, mean_eps: 0.288662\n",
      " 16062/20000: episode: 309, duration: 0.625s, episode steps: 154, steps per second: 246, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 105.875240, mae: 104.294317, mean_q: 212.217260, mean_eps: 0.280697\n",
      " 16262/20000: episode: 310, duration: 0.811s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 91.517324, mae: 105.045609, mean_q: 214.209361, mean_eps: 0.272732\n",
      " 16462/20000: episode: 311, duration: 0.811s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 85.418907, mae: 106.612412, mean_q: 216.940308, mean_eps: 0.263732\n",
      " 16662/20000: episode: 312, duration: 0.809s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 113.876233, mae: 107.067726, mean_q: 217.955852, mean_eps: 0.254732\n",
      " 16862/20000: episode: 313, duration: 0.819s, episode steps: 200, steps per second: 244, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 108.510874, mae: 107.976057, mean_q: 219.354090, mean_eps: 0.245732\n",
      " 17062/20000: episode: 314, duration: 0.820s, episode steps: 200, steps per second: 244, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 101.925422, mae: 108.067307, mean_q: 219.562454, mean_eps: 0.236732\n",
      " 17262/20000: episode: 315, duration: 0.812s, episode steps: 200, steps per second: 246, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 92.457353, mae: 109.026147, mean_q: 220.968905, mean_eps: 0.227732\n",
      " 17462/20000: episode: 316, duration: 0.813s, episode steps: 200, steps per second: 246, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 130.997134, mae: 108.678996, mean_q: 220.809850, mean_eps: 0.218732\n",
      " 17662/20000: episode: 317, duration: 0.812s, episode steps: 200, steps per second: 246, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 102.273330, mae: 109.955897, mean_q: 223.096875, mean_eps: 0.209732\n",
      " 17862/20000: episode: 318, duration: 0.815s, episode steps: 200, steps per second: 246, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 115.864705, mae: 110.217243, mean_q: 223.850675, mean_eps: 0.200732\n",
      " 18062/20000: episode: 319, duration: 0.812s, episode steps: 200, steps per second: 246, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 112.121364, mae: 111.023179, mean_q: 225.163012, mean_eps: 0.191732\n",
      " 18262/20000: episode: 320, duration: 0.826s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 105.273242, mae: 111.842949, mean_q: 227.161067, mean_eps: 0.182732\n",
      " 18462/20000: episode: 321, duration: 0.820s, episode steps: 200, steps per second: 244, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 102.655917, mae: 112.663676, mean_q: 229.069043, mean_eps: 0.173732\n",
      " 18662/20000: episode: 322, duration: 0.814s, episode steps: 200, steps per second: 246, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 110.729656, mae: 114.945149, mean_q: 232.905630, mean_eps: 0.164732\n",
      " 18862/20000: episode: 323, duration: 0.814s, episode steps: 200, steps per second: 246, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 110.164949, mae: 114.431613, mean_q: 231.841810, mean_eps: 0.155732\n",
      " 19062/20000: episode: 324, duration: 0.830s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 127.515341, mae: 114.401065, mean_q: 231.688009, mean_eps: 0.146732\n",
      " 19262/20000: episode: 325, duration: 0.814s, episode steps: 200, steps per second: 246, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 119.660365, mae: 114.648559, mean_q: 232.542455, mean_eps: 0.137732\n",
      " 19462/20000: episode: 326, duration: 0.816s, episode steps: 200, steps per second: 245, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 101.367695, mae: 113.479093, mean_q: 230.306158, mean_eps: 0.128732\n",
      " 19662/20000: episode: 327, duration: 0.816s, episode steps: 200, steps per second: 245, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 118.112051, mae: 113.034604, mean_q: 229.269063, mean_eps: 0.119732\n",
      " 19862/20000: episode: 328, duration: 0.814s, episode steps: 200, steps per second: 246, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 124.635946, mae: 112.913047, mean_q: 228.948516, mean_eps: 0.110732\n",
      "done, took 82.587 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2113ad7f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=20000, visualize=False,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights(f'my_cartpole_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
