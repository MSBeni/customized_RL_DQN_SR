{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "\n",
    "for step in range(100):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "#     print(action)\n",
    "    # sample: (array([ 0.20307615,  0.24289648, -5.11968664, -5.43760231]), 0.0, True, {})\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    time.sleep(0.02)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.24667234,  0.65470702, -4.31367009,  5.02866106]), 0.0, True, {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craete_bins(number_of_bins=10):\n",
    "    bins_cart_position = np.linspace(-4.8, 4.8, number_of_bins)\n",
    "    bins_cart_velocity = np.linspace(-5, 5, number_of_bins)\n",
    "    bins_pole_angle = np.linspace(-0.418, 0.418, number_of_bins)\n",
    "    bins_pole_angular_velocity = np.linspace(-5, 5, number_of_bins)\n",
    "    \n",
    "    bins = np.array([bins_cart_position, bins_cart_velocity, bins_pole_angle, bins_pole_angular_velocity])\n",
    "    \n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the indices of the bins to which each value in input array belongs.\n",
    "# Example:\n",
    "bins_smple = [1,2,3,4,5,6]\n",
    "np.digitize(4.4, bins_smple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 10\n",
    "all_bins = craete_bins(num_bins)\n",
    "def digitised_observation(observation):\n",
    "    digitised_bins = list()\n",
    "    for i, obs in enumerate(observation):\n",
    "        digitised_bins.append(np.digitize(obs, all_bins[i]))\n",
    "        \n",
    "    return digitised_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 0, 10]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digitised_observation([-0.24667234,  0.65470702, -4.31367009,  5.02866106])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Q-Table\n",
    "q_table = np.zeros([num_bins, num_bins, num_bins, num_bins, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 10, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# Exploration vs. Exploitation parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability\n",
    "decay_rate = 0.001             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(epsilon, q_table, discrete_state):\n",
    "    '''\n",
    "    Returns an action for the agent. Note how it uses a random number to decide on\n",
    "    exploration versus explotation trade-off.\n",
    "    '''\n",
    "    random_number = np.random.random()\n",
    "\n",
    "    # EXPLOITATION, USE BEST Q(s,a) Value\n",
    "    if random_number > epsilon:\n",
    "        # Action row for a particular state\n",
    "        state_row = q_table[discrete_state,:]\n",
    "        # Index of highest action for state\n",
    "        # Recall action is mapped to index (e.g. 0=LEFT, 1=DOWN, etc..)\n",
    "        action = np.argmax(state_row)\n",
    "\n",
    "    # EXPLORATION, USE A RANDOM ACTION\n",
    "    else:\n",
    "        # Return a random 0,1,2,3 action\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04432948,  0.01639647,  0.04121079, -0.00186996])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.003329  ,  0.19782752, -0.03872264, -0.26680626]), 1.0, False, {})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5, 5]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digitised_observation(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Process\n",
    "epoch = 200\n",
    "for episdoe in range(epoch):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    Total_Reward = 0\n",
    "    digitised_state = digitised_observation(state)\n",
    "    while not done:\n",
    "        action = epsilon_greedy_action_selection(epsilon, q_table, digitised_state)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        digitised_new_state = digitised_observation(new_state)\n",
    "        old_q = q_table[digitised_new_state, action]\n",
    "        \n",
    "        # Look up current/old qtable value Q(s_t,a_t)\n",
    "        old_q_value =  q_table[digitised_state,action]  \n",
    "\n",
    "        # Get the next optimal Q-Value\n",
    "        next_optimal_q_value = np.max(q_table[digitised_new_state, :])  \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
