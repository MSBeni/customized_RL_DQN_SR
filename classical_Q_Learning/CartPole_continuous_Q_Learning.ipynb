{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i-sip_iot/.local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "\n",
    "for step in range(100):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "#     print(action)\n",
    "    # sample: (array([ 0.20307615,  0.24289648, -5.11968664, -5.43760231]), 0.0, True, {})\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    time.sleep(0.02)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.94200436, -1.45084837, -2.09692214, -6.45398374]), 0.0, True, {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craete_bins(number_of_bins=10):\n",
    "    bins_cart_position = np.linspace(-4.8, 4.8, number_of_bins)\n",
    "    bins_cart_velocity = np.linspace(-5, 5, number_of_bins)\n",
    "    bins_pole_angle = np.linspace(-0.418, 0.418, number_of_bins)\n",
    "    bins_pole_angular_velocity = np.linspace(-5, 5, number_of_bins)\n",
    "    \n",
    "    bins = np.array([bins_cart_position, bins_cart_velocity, bins_pole_angle, bins_pole_angular_velocity])\n",
    "    \n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the indices of the bins to which each value in input array belongs.\n",
    "# Example:\n",
    "bins_smple = [1,2,3,4,5,6]\n",
    "np.digitize(4.4, bins_smple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 9\n",
    "all_bins = craete_bins(num_bins)\n",
    "def digitised_observation(observation):\n",
    "    digitised_bins = list()\n",
    "    for i, obs in enumerate(observation):\n",
    "        digitised_bins.append(np.digitize(obs, all_bins[i]))\n",
    "        \n",
    "    return tuple(digitised_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 0, 9)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digitised_observation([-0.24667234,  0.65470702, -4.31367009,  5.02866106])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Q-Table\n",
    "q_table = np.zeros([num_bins, num_bins, num_bins, num_bins, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 10, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(q_table[0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 3 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-61a0052cd0c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for axis 3 with size 10"
     ]
    }
   ],
   "source": [
    "q_table[(5, 6, 0, 10) + (1,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# Exploration vs. Exploitation parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability\n",
    "decay_rate = 0.001             # Exponential decay rate for exploration prob\n",
    "\n",
    "\n",
    "# It is common to leave Hyperparameters in ALL CAPS to easily locate them\n",
    "\n",
    "EPOCHS=20000  # number of epochs/episodes to train for\n",
    "ALPHA = 0.8 # aka the learning rate\n",
    "GAMMA = 0.95 # aka the discount rate\n",
    "# MAX_EPISODES = 100  # optional, also defined in env setup above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[(5, 6, 0, 9)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_table[(5, 6, 0, 9),:,6:,8:,9:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(epsilon, q_table, discrete_state):\n",
    "    '''\n",
    "    Returns an action for the agent. Note how it uses a random number to decide on\n",
    "    exploration versus explotation trade-off.\n",
    "    '''\n",
    "    random_number = np.random.random()\n",
    "\n",
    "    # EXPLOITATION, USE BEST Q(s,a) Value\n",
    "    if random_number > epsilon:\n",
    "        # Action row for a particular state\n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "\n",
    "    # EXPLORATION, USE A RANDOM ACTION\n",
    "    else:\n",
    "        # Return a random 0,1 action\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "BURN_IN = 1\n",
    "EPSILON_END = 10000\n",
    "EPSILON_REDUCE = 0.0001\n",
    "\n",
    "def reduce_epsilon(epsilon,epoch):\n",
    "    if BURN_IN <= epoch <= EPSILON_END:\n",
    "        epsilon -= EPSILON_REDUCE\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fail(done, points, reward):\n",
    "    if done and points < 150:\n",
    "        reward = -200\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_next_q_value(old_q_value, reward, next_optimal_q_value):\n",
    "\n",
    "    return old_q_value +  ALPHA * (reward + GAMMA * next_optimal_q_value - old_q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02973516,  0.04505068, -0.01412676, -0.01070191])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03063617,  0.24037235, -0.0143408 , -0.30780831]), 1.0, False, {})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 5, 5)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digitised_observation(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Process\n",
    "points_log = []  # to store all achieved points\n",
    "epochs = []  # store the epoch for plotting\n",
    "rewards = []\n",
    "epoch = 200\n",
    "for episode in range(EPOCHS):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    epochs.append(epoch)\n",
    "    points = 0  # store result\n",
    "    digitised_state = digitised_observation(state)\n",
    "    while not done:\n",
    "        action = epsilon_greedy_action_selection(epsilon, q_table, digitised_state)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        reward = fail(done, points, reward)  # Check if reward or fail state\n",
    "        digitised_new_state = digitised_observation(new_state)\n",
    "        \n",
    "        # Look up current/old qtable value Q(s_t,a_t)\n",
    "        old_q_value =  q_table[digitised_state + (action,)]  \n",
    "\n",
    "        # Get the next optimal Q-Value\n",
    "        next_optimal_q_value = np.max(q_table[digitised_new_state])  \n",
    "#         print(\"next_optimal_q_value: \", next_optimal_q_value)\n",
    "        \n",
    "        # Compute next q value\n",
    "        next_q = compute_next_q_value(old_q_value, reward, next_optimal_q_value) \n",
    "#         print(\"next_q: \", next_q)\n",
    "\n",
    "        # Update Q Table\n",
    "        q_table[digitised_state + (action,)] = next_q\n",
    "        \n",
    "        points += 1\n",
    "        \n",
    "                \n",
    "        # Our new state is state\n",
    "        digitised_state = digitised_new_state\n",
    "        \n",
    "#     episode += 1\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = reduce_epsilon(epsilon,episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "\n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You got 200 points!\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "rewards = 0\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    discrete_state = digitised_observation(observation)  # get bins\n",
    "    action = np.argmax(q_table[discrete_state])  # and chose action from the Q-Table\n",
    "    observation, reward, done, info = env.step(action) # Finally perform the action\n",
    "    rewards+=1\n",
    "    if done:\n",
    "        print(f\"You got {rewards} points!\")\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
