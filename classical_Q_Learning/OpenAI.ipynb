{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/4b/48/920cea66177b865663fde5a9390a59de0ef3b642ad98106ac1d8717d7005/gym-0.21.0.tar.gz (1.5MB)\n",
      "\u001B[K     |████████████████████████████████| 1.5MB 6.3MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.1)\n",
      "Collecting cloudpickle>=1.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/07/3c/bf72ebd3e78eb1ef773f4f0650ecdc29c6454aeafe9c08f6da3f227dd2bc/cloudpickle-2.0.0-py3-none-any.whl\n",
      "Collecting importlib_metadata>=4.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/71/c2/cb1855f0b2a0ae9ccc9b69f150a7aebd4a8d815bd951e74621c4154c52a8/importlib_metadata-4.8.1-py3-none-any.whl\n",
      "Collecting typing-extensions>=3.6.4; python_version < \"3.8\"\n",
      "  Downloading https://files.pythonhosted.org/packages/74/60/18783336cc7fcdd95dae91d73477830aa53f5d3181ae4fe20491d7fc3199/typing_extensions-3.10.0.2-py3-none-any.whl\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib_metadata>=4.8.1->gym) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib_metadata>=4.8.1->gym) (8.0.2)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for gym: filename=gym-0.21.0-cp36-none-any.whl size=1617969 sha256=3469f6bbcd20db687fdfb9a1c7c180f57c7f2af05a0310374f2d73a81604a5aa\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/38/eb/6f4627557fa3c77cc4b07654d299dfb1c43d5dd3ad966ea6c7\n",
      "Successfully built gym\n",
      "Installing collected packages: cloudpickle, typing-extensions, importlib-metadata, gym\n",
      "  Found existing installation: importlib-metadata 1.4.0\n",
      "    Uninstalling importlib-metadata-1.4.0:\n",
      "      Successfully uninstalled importlib-metadata-1.4.0\n",
      "Successfully installed cloudpickle-2.0.0 gym-0.21.0 importlib-metadata-4.8.1 typing-extensions-3.10.0.2\n",
      "\u001B[33mWARNING: You are using pip version 19.3.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mineral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Breakout env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "try:\n",
    "\n",
    "    register(\n",
    "        id='FrozenLakeNotSlippery-v0', # make sure this is a custom name!\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=.8196, # optimum = .8196\n",
    "    )\n",
    "except:\n",
    "    print('You probably ran this cell twice, accidentally trying to register a new env with the same id twice.')\n",
    "    print(\"Either change the id, or just continue, knowing your id was already registered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnregisteredEnv",
     "evalue": "No registered env with id: Breakout-v0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\u001B[0m in \u001B[0;36mspec\u001B[0;34m(self, path)\u001B[0m\n\u001B[1;32m    157\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 158\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv_specs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    159\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Breakout-v0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mUnregisteredEnv\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-77ec9cff7058>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0menv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menv_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\u001B[0m in \u001B[0;36mmake\u001B[0;34m(id, **kwargs)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    234\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mmake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 235\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mregistry\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    236\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\u001B[0m in \u001B[0;36mmake\u001B[0;34m(self, path, **kwargs)\u001B[0m\n\u001B[1;32m    126\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Making new env: %s\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 128\u001B[0;31m         \u001B[0mspec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspec\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    129\u001B[0m         \u001B[0menv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspec\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\u001B[0m in \u001B[0;36mspec\u001B[0;34m(self, path)\u001B[0m\n\u001B[1;32m    201\u001B[0m                 )\n\u001B[1;32m    202\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 203\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0merror\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mUnregisteredEnv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"No registered env with id: {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    204\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    205\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mUnregisteredEnv\u001B[0m: No registered env with id: Breakout-v0"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")  # Load FrozenLake\n",
    "env.reset()  # Reset to initial state\n",
    "for _ in range(5):\n",
    "    env.render()  # Render on the screen\n",
    "    action = env.action_space.sample()  # chose a random action\n",
    "    env.step(action)  # Perform random action on the environment\n",
    "env.close()  # dont forget to close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Manually Play the Environment\n",
    "\n",
    "# OPTIONAL: Not visually pleasing to see a list, let's clear the output of the cell before writing the new state. For Jupyter Notebook users use the code shown in the cell below, for .py script use:**\n",
    "\n",
    "    import os\n",
    "    os.system('cls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")  # Load FrozenLake\n",
    "observation = env.reset()  # Reset to initial state\n",
    "for epsiode in range(4):\n",
    "    print(observation)\n",
    "    env.render()  # Render on the screen\n",
    "    time.sleep(0.2)\n",
    "    clear_output(wait=True)\n",
    "    action = env.action_space.sample()  # chose a random action\n",
    "    observation, reward, done, info = env.step(action)  # Perform random action on the environment\n",
    "\n",
    "env.close()  # dont forget to close the environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us take a look at the possible actions.\n",
    "You can find them here: https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\n",
    "\n",
    "As we can see, there is just 1 observation, a single digit representing the current location, for a 4by4 \"lake\":\n",
    "\n",
    "    0    1   2  3\n",
    "    4    5   6  7\n",
    "    8    9  10  11\n",
    "    12  13  14  15\n",
    "\n",
    "and four actions:\n",
    "\n",
    "    LEFT = 0\n",
    "    DOWN = 1\n",
    "    RIGHT = 2\n",
    "    UP = 3\n",
    "\n",
    "The next cell contains code to play the game manually - Feel free to try it out by using the left and right arrow key.\n",
    "\n",
    "### Very simple code to manually play. This will break if you don't use asdw() function correctly.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# PART 3:\n",
    "# Hyperparameters\n",
    "\n",
    "**The Q-Learning update functions will require hyperparameters. we'll define them here. Often the best place to choose a good starting value is reading publications or through experimentation. Unfortunately, its very difficult to give general advice, as most environments are radically different to each other, and often hyperparameter tuning is required.**\n",
    "\n",
    "**Q-Learning Equation Parameters: https://en.wikipedia.org/wiki/Q-learning**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# It is common to leave Hyperparameters in ALL CAPS to easily locate them\n",
    "\n",
    "EPOCHS=20000  # number of epochs/episodes to train for\n",
    "ALPHA = 0.8 # aka the learning rate\n",
    "GAMMA = 0.95 # aka the discount rate\n",
    "# MAX_EPISODES = 100  # optional, also defined in env setup above"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Exploration vs. Exploitation Parameters**\n",
    "\n",
    "Basically how fast do we reduce epsilon. Reduce too fast, agent won't have enough time to learn. Reduce too slow, you're wasting time picking random actions. Key here is that these value help balance exploration (random choice) versus explotation (always picking what works for that Q(s,a). It's a tough balance!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Exploration vs. Exploitation parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability\n",
    "decay_rate = 0.001             # Exponential decay rate for exploration prob\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now it is time to dive into the training / Q-Table update methodology.<br />\n",
    "First we will define some functions needed for training phase\n",
    "\n",
    "* epsilon_greedy_action_selection: Is used to implement the epsilon greedy action selection routine.\n",
    "* compute_next_q_value: Computes the next Q-Values according to the formula from the lecture\n",
    "* reduce_epsilon: Reduces the $\\epsilon$ used for the epsilon greedy algorithm\n",
    "\n",
    "**FUNCTION TO SELECT AN ACTION**\n",
    "\n",
    "If we simply always select the argmax() qtable value during training, we'll most likely get stuck in an explotation loop, so we'll use a random value to randomly select an action from time to time, helping the model explore , rather than exploit.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(epsilon, q_table, discrete_state):\n",
    "    '''\n",
    "    Returns an action for the agent. Note how it uses a random number to decide on\n",
    "    exploration versus explotation trade-off.\n",
    "    '''\n",
    "    random_number = np.random.random()\n",
    "\n",
    "    # EXPLOITATION, USE BEST Q(s,a) Value\n",
    "    if random_number > epsilon:\n",
    "        # Action row for a particular state\n",
    "        state_row = q_table[discrete_state,:]\n",
    "        # Index of highest action for state\n",
    "        # Recall action is mapped to index (e.g. 0=LEFT, 1=DOWN, etc..)\n",
    "        action = np.argmax(state_row)\n",
    "\n",
    "    # EXPLORATION, USE A RANDOM ACTION\n",
    "    else:\n",
    "        # Return a random 0,1,2,3 action\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "    return action\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**FUNCTION FOR Q_VALUE COMPUTATION**\n",
    "\n",
    "**Here we have our main Q-Learning update equation, note how it takes in the old q-value, the next optimal q value, along with our current reward, and then updates the next q value accordingly.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_next_q_value(old_q_value, reward, next_optimal_q_value):\n",
    "\n",
    "    return old_q_value +  ALPHA * (reward + GAMMA * next_optimal_q_value - old_q_value)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**FUNCTION TO REDUCE EPSILON**\n",
    "\n",
    "**As training continues, we need to balance explotation versus exploration, we want ot make sure our agent doesn't get trapped in a cycle going from an F square to another F Square back and forth. We also don't want our agent permanently choosing random values. We'll use the function below to try to balance this.**\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reduce_epsilon(epsilon,epoch):\n",
    "\n",
    "    return min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-0e91eb3bfb5e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mq_table\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstate_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction_size\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mtotal_reward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mepsilon\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros([state_size, action_size])\n",
    "total_reward = 0\n",
    "epsilon = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "# Play 20k games\n",
    "for episode in range(EPOCHS):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = epsilon_greedy_action_selection(epsilon,q_table, state)\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        \n",
    "        # Look up current/old qtable value Q(s_t,a_t)\n",
    "        old_q_value =  q_table[state,action]  \n",
    "\n",
    "        # Get the next optimal Q-Value\n",
    "        next_optimal_q_value = np.max(q_table[new_state, :])  \n",
    "\n",
    "        # Compute next q value\n",
    "        next_q = compute_next_q_value(old_q_value, reward, next_optimal_q_value)   \n",
    "\n",
    "        # Update Q Table\n",
    "        q_table[state,action] = next_q\n",
    "\n",
    "        \n",
    "        \n",
    "        total_rewards = total_rewards + reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "\n",
    "        \n",
    "    episode += 1\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = reduce_epsilon(epsilon,episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "\n",
    "env.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(range(EPOCHS),np.cumsum(rewards))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}