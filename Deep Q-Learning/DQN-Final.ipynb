{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# for step in range(500):\n",
    "#     env.render(mode='human')\n",
    "#     random_action = env.action_space.sample()\n",
    "#     env.step(random_action)\n",
    "    \n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 observations we have in cartpole\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observations = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape of the ANN should be equal to number of observation --> 4 here\n",
    "# output size of the ANN should be equal to number of actions --> 2 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# by adding the input_shape we become sure that we match currently to the firt layer regardless of num of obs\n",
    "model.add(Dense(16, input_shape=(1, num_observations)))\n",
    "# model.add(Dense(16))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "# Neurons == action_space\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation('linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1, 16)             80        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1, 16)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1, 32)             544       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1, 32)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1, 2)              66        \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 1, 2)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = clone_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now these tasks ought to be done:\n",
    "#     1- Define Hyper parameters\n",
    "#     2- Epsilon greedy action selection\n",
    "#     3- Undersatnd Deque Object\n",
    "#     4- Create Replay Function\n",
    "#     5- Tartget Model Update Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "epsilon = 1.0\n",
    "EPSILON_REDUCE = 0.995\n",
    "\n",
    "LEARNIN_RATE = 0.001\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(model, epsilon, observation, batch_size=32):\n",
    "    if np.random.random() > epsilon:\n",
    "\n",
    "        prediction = model.predict(observation.reshape([1, 1, 4]))\n",
    "        action = np.argmax(prediction)\n",
    "    else:\n",
    "        action = np.random.randint(0, env.action_space.n)\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deque add a limit to the number of values that can be added to it and once it reach the threshold it will stop adding elements\n",
    "# and if continue appending items to it it will remove the very first one and add th last one\n",
    "deque_1 = deque(maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deque_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([0], maxlen=5)\n",
      "deque([0, 1], maxlen=5)\n",
      "deque([0, 1, 2], maxlen=5)\n",
      "deque([0, 1, 2, 3], maxlen=5)\n",
      "deque([0, 1, 2, 3, 4], maxlen=5)\n",
      "deque([1, 2, 3, 4, 5], maxlen=5)\n",
      "deque([2, 3, 4, 5, 6], maxlen=5)\n",
      "deque([3, 4, 5, 6, 7], maxlen=5)\n",
      "deque([4, 5, 6, 7, 8], maxlen=5)\n",
      "deque([5, 6, 7, 8, 9], maxlen=5)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    deque_1.append(i)\n",
    "    print(deque_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the deque to define replay buffer\n",
    "replay_buffer = deque(maxlen=20000)\n",
    "update_target_model = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(replay_buffer, batch_size, model, target_model):\n",
    "    if len(replay_buffer)<batch_size:\n",
    "        return\n",
    "   \n",
    "    samples = random.sample(replay_buffer, batch_size)\n",
    "\n",
    "    target_batch = list()\n",
    "    zipped_samples = list(zip(*samples))\n",
    "\n",
    "    states, actions, rewards, new_states, dones = zipped_samples\n",
    "    states = np.array(states).reshape([batch_size, 1, 4])\n",
    "    new_states = np.array(new_states).reshape([batch_size, 1, 4])\n",
    "    targets = target_model.predict(states)\n",
    "    # predict q values for all the samples\n",
    "#     q_values = model.predict(np.array(new_states))\n",
    "    q_values = model.predict(new_states)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        q_value = max(q_values[i][0])\n",
    "        target = targets[i].copy()\n",
    "        if dones[i]:\n",
    "            target[0][actions[i]] = rewards[i]\n",
    "        else:\n",
    "            target[0][actions[i]] = rewards[i]+q_value*GAMMA\n",
    "        target_batch.append(target)\n",
    "\n",
    "    model.fit(np.array(states), np.array(target_batch), epochs=1, verbose=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_handler(epoch, update_target_model, model, target_moel):\n",
    "    if epoch > 0 and epoch%update_target_model==0:\n",
    "        target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salimibeni\\anaconda3\\envs\\gymTF\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=(Adam(lr=LEARNIN_RATE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1, 16)             80        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1, 16)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1, 32)             544       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1, 32)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1, 2)              66        \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 1, 2)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: POINTS: 18 eps: 0.995 BSF: 18\n",
      "25: POINTS: 13 eps: 0.8778091417340573 BSF: 72\n",
      "50: POINTS: 29 eps: 0.7744209942832988 BSF: 72\n",
      "75: POINTS: 14 eps: 0.6832098777212641 BSF: 72\n",
      "100: POINTS: 17 eps: 0.6027415843082742 BSF: 72\n",
      "125: POINTS: 15 eps: 0.531750826943791 BSF: 72\n",
      "150: POINTS: 13 eps: 0.46912134373457726 BSF: 72\n",
      "175: POINTS: 10 eps: 0.41386834584198684 BSF: 72\n",
      "200: POINTS: 13 eps: 0.36512303261753626 BSF: 72\n",
      "225: POINTS: 9 eps: 0.322118930542046 BSF: 72\n",
      "250: POINTS: 10 eps: 0.28417984116121187 BSF: 72\n",
      "275: POINTS: 9 eps: 0.2507092085103961 BSF: 72\n",
      "300: POINTS: 11 eps: 0.2211807388415433 BSF: 72\n",
      "325: POINTS: 12 eps: 0.19513012515638165 BSF: 72\n",
      "350: POINTS: 10 eps: 0.17214774642209296 BSF: 72\n",
      "375: POINTS: 11 eps: 0.1518722266715875 BSF: 72\n",
      "400: POINTS: 8 eps: 0.13398475271138335 BSF: 72\n",
      "425: POINTS: 11 eps: 0.11820406108847166 BSF: 72\n",
      "450: POINTS: 10 eps: 0.1042820154910064 BSF: 72\n",
      "475: POINTS: 9 eps: 0.09199970504166631 BSF: 72\n",
      "500: POINTS: 9 eps: 0.0811640021330769 BSF: 72\n",
      "525: POINTS: 9 eps: 0.0716045256805401 BSF: 72\n",
      "550: POINTS: 9 eps: 0.06317096204211972 BSF: 72\n",
      "575: POINTS: 11 eps: 0.05573070148010834 BSF: 72\n",
      "600: POINTS: 9 eps: 0.04916675299948831 BSF: 72\n",
      "625: POINTS: 10 eps: 0.043375904776212296 BSF: 72\n",
      "650: POINTS: 10 eps: 0.03826710124979409 BSF: 72\n",
      "675: POINTS: 9 eps: 0.033760011361539714 BSF: 72\n",
      "700: POINTS: 9 eps: 0.029783765425331846 BSF: 72\n",
      "725: POINTS: 8 eps: 0.026275840769466357 BSF: 72\n",
      "750: POINTS: 10 eps: 0.023181078627322618 BSF: 72\n",
      "775: POINTS: 9 eps: 0.020450816818411825 BSF: 72\n",
      "800: POINTS: 10 eps: 0.018042124582040707 BSF: 72\n",
      "825: POINTS: 10 eps: 0.015917127532080494 BSF: 72\n",
      "850: POINTS: 10 eps: 0.014042412118399107 BSF: 72\n",
      "875: POINTS: 9 eps: 0.012388500230681249 BSF: 72\n",
      "900: POINTS: 10 eps: 0.010929385683282892 BSF: 72\n",
      "925: POINTS: 11 eps: 0.009642125292786984 BSF: 72\n",
      "950: POINTS: 10 eps: 0.008506478118345316 BSF: 72\n",
      "975: POINTS: 9 eps: 0.007504587192205264 BSF: 72\n"
     ]
    }
   ],
   "source": [
    "best_so_far = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    observation = env.reset()\n",
    "    # (1, X) X observation size here is 4 we resgape to be (1, 4)\n",
    "    obsrvation = observation.reshape([1, 4])\n",
    "    done = False\n",
    "    \n",
    "    points = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = epsilon_greedy_action_selection(model, epsilon, observation)\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        next_observation = next_observation.reshape([1, 4])\n",
    "        replay_buffer.append((observation, action, reward, next_observation, done))\n",
    "        observatin = next_observation\n",
    "        \n",
    "        points += 1\n",
    "        \n",
    "        replay(replay_buffer, BATCH_SIZE, model, target_model)\n",
    "    \n",
    "    epsilon *= EPSILON_REDUCE # eps*0.995\n",
    "    update_model_handler(epoch, update_target_model, model, target_model)\n",
    "    if points > best_so_far:\n",
    "        best_so_far = points\n",
    "        \n",
    "    if epoch%25 == 0:\n",
    "        print(f\"{epoch}: POINTS: {points} eps: {epsilon} BSF: {best_so_far}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "for counter in range(300):\n",
    "    env.render()\n",
    "    \n",
    "    # TODO: Get discretized observation\n",
    "    action = np.argmax(model.predict(observation.reshape([1, 1, 4])))\n",
    "    \n",
    "    # TODO: Perform the action \n",
    "    observation, reward, done, info = env.step(action) # Finally perform the action\n",
    "    \n",
    "    if done:\n",
    "        print(f\"done\")\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
